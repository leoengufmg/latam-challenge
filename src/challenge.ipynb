{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kas7oVne358i"
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import io\n",
    "\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yao7hjGzjbZb"
   },
   "outputs": [],
   "source": [
    "# configure ->\n",
    "\n",
    "# Constant values\n",
    "bucket_name = 'gcp_latam_twitter'\n",
    "folder_name = 'raw'\n",
    "zip_file_name = 'tweets.json.zip'\n",
    "\n",
    "# ID de ejemplo del archivo de Google Drive\n",
    "file_id = '1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis'\n",
    "gcloud_url = f\"gs://{bucket_name}/{folder_name}/\"\n",
    "start_time = str(time.time())\n",
    "\n",
    "# Record the start time\n",
    "start_time = str(time.time())\n",
    "\n",
    "\n",
    "# Define local file paths\n",
    "drive_mount_point = '/content/drive/MyDrive'\n",
    "source_path = 'leonardora/de/latam-challenge/'\n",
    "\n",
    "# Set Google Cloud project and dataset info\n",
    "project_id = \"latam-challenge-leonardora\"\n",
    "project_name = \"latam-challenge-leonardora\"\n",
    "dataset = \"latam_tweets_dataset\"\n",
    "table =\"tweets\"\n",
    "\n",
    "# Logging\n",
    "logging_level = str(logging.DEBUG)\n",
    "logging.basicConfig(level=int(logging_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VCfmNjJa4cvC"
   },
   "outputs": [],
   "source": [
    "# Reloads all modules automatically before executing a new line, so your latest changes are always available\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaxHvatPfkVU",
    "outputId": "fc85a9c3-bf7b-47f5-fa90-0b7feecbb4f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Directorio existente: /content/drive/MyDrive/leonardora/de/latam-challenge/\n",
      "Directorio actual: /content/drive/MyDrive/leonardora/de/latam-challenge\n",
      "Repositorio ya existe. Haciendo pull de los últimos cambios...\n",
      "M\trequirements.txt\n",
      "Already on 'develop'\n",
      "Your branch and 'origin/develop' have diverged,\n",
      "and have 5 and 30 different commits each, respectively.\n",
      "  (use \"git pull\" to merge the remote branch into yours)\n",
      "From https://github.com/leoengufmg/latam-challenge\n",
      " * branch            develop    -> FETCH_HEAD\n",
      "\u001b[33mhint: You have divergent branches and need to specify how to reconcile them.\u001b[m\n",
      "\u001b[33mhint: You can do so by running one of the following commands sometime before\u001b[m\n",
      "\u001b[33mhint: your next pull:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase false  # merge (the default strategy)\u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase true   # rebase\u001b[m\n",
      "\u001b[33mhint:   git config pull.ff only       # fast-forward only\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: You can replace \"git config\" with \"git config --global\" to set a default\u001b[m\n",
      "\u001b[33mhint: preference for all repositories. You can also pass --rebase, --no-rebase,\u001b[m\n",
      "\u001b[33mhint: or --ff-only on the command line to override the configured default per\u001b[m\n",
      "\u001b[33mhint: invocation.\u001b[m\n",
      "fatal: Need to specify how to reconcile divergent branches.\n"
     ]
    }
   ],
   "source": [
    "# test.py ->\n",
    "from google.colab import drive\n",
    "import os\n",
    "import logging\n",
    "import subprocess\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "drive_mount_point = '/content/drive/MyDrive'\n",
    "source_path = 'leonardora/de/latam-challenge/'\n",
    "target_dir = os.path.join(drive_mount_point, source_path)\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "    print(f\"Directorio creado: {target_dir}\")\n",
    "else:\n",
    "    print(f\"Directorio existente: {target_dir}\")\n",
    "\n",
    "os.chdir(target_dir)\n",
    "print(f\"Directorio actual: {os.getcwd()}\")\n",
    "\n",
    "if os.path.exists(os.path.join(target_dir, \".git\")):\n",
    "    print(\"Repositorio ya existe. Haciendo pull de los últimos cambios...\")\n",
    "    !git checkout develop\n",
    "    !git pull origin develop\n",
    "else:\n",
    "    repo_url = \"https://github.com/leoengufmg/latam-challenge.git\"\n",
    "    print(\"Clonando el repositorio...\")\n",
    "    !git clone {repo_url} .\n",
    "\n",
    "    !git checkout develop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I8VPlQ8Imp35"
   },
   "outputs": [],
   "source": [
    "# Test ->\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Uk3rCWQtgOT9"
   },
   "outputs": [],
   "source": [
    "# Functions.py ->\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_requirements(requirements_path: str = \"./requirements.txt\") -> bool:\n",
    "    \"\"\"\n",
    "    Installs libraries listed in the requirements file if they are not already installed.\n",
    "\n",
    "    Args:\n",
    "        requirements_path (str): Path to the requirements.txt file. Defaults to \"./requirements.txt\".\n",
    "\n",
    "    Returns:\n",
    "        bool: True if installation is successful or libraries are already installed, False if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(requirements_path, 'r') as file:\n",
    "            requirements = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "        for requirement in requirements:\n",
    "            print(f\"Installing {requirement}...\")\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", requirement], check=True)\n",
    "            print(f\"{requirement} installed successfully.\")\n",
    "\n",
    "        print(\"All required libraries were installed successfully.\")\n",
    "        return True\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error installing libraries: {e}\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Requirements file not found at: {requirements_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HplLv9HQdtXC",
    "outputId": "5ad1e624-9c8d-4f86-fa96-7678a5171342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing memory-profiler==0.61.0...\n",
      "memory-profiler==0.61.0 installed successfully.\n",
      "Installing google_cloud_bigquery==3.20.1...\n",
      "google_cloud_bigquery==3.20.1 installed successfully.\n",
      "Installing line_profiler==4.1.2...\n",
      "line_profiler==4.1.2 installed successfully.\n",
      "All required libraries were installed successfully.\n",
      "Procediendo a descargar datos...\n"
     ]
    }
   ],
   "source": [
    "# test -> installing requirements\n",
    "if install_requirements():\n",
    "    print(\"Procediendo a descargar datos...\")\n",
    "    # Código para descargar datos u otras acciones adicionales\n",
    "else:\n",
    "    print(\"Error al instalar los requisitos. Abortando acciones adicionales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSV8gzel2WbH",
    "outputId": "580c4386-f740-4f22-9c6c-0551274ed019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket gcp_latam_twitter already exists.\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Function.py -> setup bucket\n",
    "from google.cloud import storage\n",
    "from google.api_core.exceptions import Conflict\n",
    "\n",
    "def create_bucket_if_not_exists(bucket_name: str, project_id: str, location: str = \"US\") -> storage.Bucket:\n",
    "    \"\"\"Creates a new bucket in Google Cloud Storage if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The unique name of the bucket to create.\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        location (str): The location where the bucket will be created (default is \"US\").\n",
    "\n",
    "    Returns:\n",
    "        storage.Bucket: The bucket object, either newly created or already existing.\n",
    "    \"\"\"\n",
    "    # Initialize the Google Cloud Storage client\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "\n",
    "    # Check if the bucket already exists\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "        return bucket\n",
    "    except Exception:\n",
    "        print(f\"Bucket {bucket_name} does not exist, attempting to create it.\")\n",
    "\n",
    "    # Create the bucket if it does not exist\n",
    "    try:\n",
    "        bucket = storage_client.create_bucket(bucket_name, location=location)\n",
    "        print(f\"Bucket {bucket.name} created in {location}.\")\n",
    "        return bucket\n",
    "    except Conflict:\n",
    "        print(f\"Bucket {bucket_name} already exists (Conflict error).\")\n",
    "        return storage_client.get_bucket(bucket_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating bucket: {e}\")\n",
    "        raise\n",
    "\n",
    "new_bucket = create_bucket_if_not_exists(bucket_name, project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "udvTNbkSgPel"
   },
   "outputs": [],
   "source": [
    "# functions.py ->\n",
    "import io\n",
    "import logging\n",
    "from google.colab import auth\n",
    "from google.colab import drive\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.cloud import storage  # For Google Cloud Storage access\n",
    "from typing import Any\n",
    "\n",
    "# Configura el log\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Autentica en Google Colab\n",
    "def authenticate_google_drive() -> None:\n",
    "    \"\"\"Authenticate the user with Google Drive.\"\"\"\n",
    "    auth.authenticate_user()\n",
    "\n",
    "def mount_google_drive(mount_point: str = '/content/drive') -> None:\n",
    "    \"\"\"Mounts Google Drive to a specified mount point.\"\"\"\n",
    "    drive.mount(mount_point, force_remount=True)\n",
    "\n",
    "def download_file_from_drive(drive_service: Any, file_id: str) -> io.BytesIO:\n",
    "    \"\"\"Downloads a file from Google Drive and returns it as a BytesIO object.\"\"\"\n",
    "    downloaded = io.BytesIO()\n",
    "    try:\n",
    "        request = drive_service.files().get_media(fileId=file_id)\n",
    "        downloader = MediaIoBaseDownload(downloaded, request)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f'Downloading {int(status.progress() * 100)}%')\n",
    "\n",
    "        downloaded.seek(0)\n",
    "        return downloaded\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading file: {e}\")\n",
    "        raise\n",
    "\n",
    "def upload_drive_file_to_cloud_storage(bucket: storage.Bucket, folder_name: str, file_data: io.BytesIO, file_name: str) -> storage.Blob:\n",
    "    \"\"\"Uploads a file to Google Cloud Storage.\"\"\"\n",
    "    blob = bucket.blob(f\"{folder_name}/{file_name}\")\n",
    "    file_data.seek(0)  # Resetea el puntero del archivo\n",
    "    blob.upload_from_file(file_data)\n",
    "    return blob\n",
    "\n",
    "\n",
    "def extract_zip_file_conditionally(\n",
    "    bucket: storage.Bucket, folder_name: str, zip_file_name: str\n",
    ") -> str:\n",
    "    \"\"\"Extracts a ZIP file in Google Cloud Storage conditionally.\n",
    "\n",
    "    Checks if the ZIP file exists and is identical to the local version before\n",
    "    extracting its contents to avoid redundant operations.\n",
    "\n",
    "    Args:\n",
    "        bucket (storage.Bucket): The Google Cloud Storage bucket object.\n",
    "        folder_name (str): The name of the folder containing the ZIP file.\n",
    "        zip_file_name (str): The name of the ZIP file to extract.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the extracted JSON file, or an empty string if no\n",
    "              extraction occurred.\n",
    "    \"\"\"\n",
    "    json_file_name = ''\n",
    "    blob_name = ''\n",
    "\n",
    "    try:\n",
    "        # Verify ZIP file existence in the bucket\n",
    "        zip_blob = bucket.blob(f'{folder_name}/{zip_file_name}')\n",
    "        if not zip_blob.exists():\n",
    "            print(f\"ZIP file '{zip_file_name}' does not exist in bucket '{bucket.name}'.\")\n",
    "            return False\n",
    "\n",
    "        # Open the ZIP archive in memory for efficient processing\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_blob.download_as_string()), 'r') as z:\n",
    "            for file_info in z.infolist():  # Iterate through each file in the ZIP archive\n",
    "                blob_name = f'{folder_name}/{file_info.filename}'  # Construct blob path\n",
    "                json_file_name = file_info.filename  # Store the JSON file name\n",
    "                json_blob = bucket.blob(blob_name)\n",
    "\n",
    "                # Download as string and get the size if the JSON blob exists in the bucket\n",
    "                if json_blob.exists():\n",
    "                    existing_blob_data: str = json_blob.download_as_string()\n",
    "                    existing_blob_size: int = len(existing_blob_data)\n",
    "\n",
    "                # Check for file existence and size match for conditional extraction\n",
    "                if json_blob.exists() and existing_blob_size == file_info.file_size:\n",
    "                    print(f\"File '{json_file_name}' already exists on cloud storage with exact matching size, skipping extraction.\")\n",
    "                else:\n",
    "                    # Extract and upload the file if conditions are not met\n",
    "                    with z.open(file_info) as file:\n",
    "                        json_blob.upload_from_file(file)  # Upload extracted file\n",
    "\n",
    "                    print(f'ZIP File extracted to gs://{bucket.name}/{blob_name}')\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        logging.warning(f'Invalid ZIP file: gs://{bucket.name}/{folder_name}/{zip_file_name}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error extracting ZIP file: {e}')\n",
    "\n",
    "    finally:\n",
    "        return json_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3AbVBvL7wsp",
    "outputId": "0b394ef2-81de-4cfe-d979-8d569e0865aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Downloading 100%\n",
      "File transfer process completed.\n"
     ]
    }
   ],
   "source": [
    "# Inicializa el objeto para el archivo descargado\n",
    "downloaded: io.BytesIO = io.BytesIO()\n",
    "\n",
    "try:\n",
    "    # Autenticación y montaje de Google Drive\n",
    "    authenticate_google_drive()\n",
    "    mount_google_drive()\n",
    "    drive_service: Any = build('drive', 'v3')\n",
    "\n",
    "    # Acceso a Google Cloud Storage\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    downloaded = download_file_from_drive(drive_service, file_id)\n",
    "\n",
    "    # Verificar si el archivo tiene contenido\n",
    "    if downloaded.getbuffer().nbytes == 0:\n",
    "        logging.info(\"Skipping upload as the file is empty.\")\n",
    "    else:\n",
    "        # Cargar el archivo en Google Cloud Storage\n",
    "        uploaded_blob = upload_drive_file_to_cloud_storage(bucket, folder_name, downloaded, zip_file_name)\n",
    "\n",
    "        # Descomprimir el archivo si es un ZIP\n",
    "        json_file_name: str = extract_zip_file_conditionally(bucket, folder_name, zip_file_name)\n",
    "\n",
    "    logging.info(\"File transfer successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    downloaded.close()\n",
    "    print(\"File transfer process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wQYAgCO4lxfD"
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "\n",
    "def authenticate_bigquery(project_id: str) -> bigquery.Client:\n",
    "    \"\"\"\n",
    "    Authenticates to BigQuery and returns the client object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        logging.info(f\"Authenticated to BigQuery using project ID '{project_id}'.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Authentication to BigQuery failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_dataset(client: bigquery.Client, dataset_id: str, mode: str = \"create\") -> None:\n",
    "    \"\"\"\n",
    "    Creates a BigQuery dataset, with options for existence checks and overwriting.\n",
    "    \"\"\"\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "    try:\n",
    "        client.get_dataset(dataset_ref)\n",
    "        if mode == \"overwrite\":\n",
    "            logging.info(f\"Overwriting dataset '{dataset_id}'...\")\n",
    "            client.delete_dataset(dataset_ref, delete_contents=True)\n",
    "            client.create_dataset(dataset_ref)\n",
    "            logging.info(f\"Dataset '{dataset_id}' overwritten.\")\n",
    "        else:\n",
    "            logging.info(f\"Dataset '{dataset_id}' already exists. Skipping creation.\")\n",
    "    except NotFound:\n",
    "        logging.info(f\"Dataset '{dataset_id}' not found, creating...\")\n",
    "        client.create_dataset(dataset_ref)\n",
    "        logging.info(f\"Dataset '{dataset_id}' created.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error managing dataset '{dataset_id}': {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_table(client: bigquery.Client, dataset_id: str, table_name: str, mode: str = \"create\") -> None:\n",
    "    \"\"\"\n",
    "    Creates a BigQuery table, with options for existence checks and overwriting.\n",
    "    \"\"\"\n",
    "    table_ref = client.dataset(dataset_id).table(table_name)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        if mode == \"overwrite\":\n",
    "            logging.info(f\"Overwriting table '{table_name}'...\")\n",
    "            client.delete_table(table_ref)\n",
    "            client.create_table(bigquery.Table(table_ref))\n",
    "            logging.info(f\"Table '{table_name}' overwritten.\")\n",
    "        else:\n",
    "            logging.info(f\"Table '{table_name}' already exists. Skipping creation.\")\n",
    "    except NotFound:\n",
    "        logging.info(f\"Table '{table_name}' not found, creating...\")\n",
    "        client.create_table(bigquery.Table(table_ref))\n",
    "        logging.info(f\"Table '{table_name}' created.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error managing table '{table_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_data_from_storage(\n",
    "    client: bigquery.Client,\n",
    "    source_uri: str,\n",
    "    dataset_name: str,\n",
    "    table_name: str,\n",
    "    json_file_name: str\n",
    ") -> None:\n",
    "\n",
    "    full_source_uri = source_uri + json_file_name\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "        autodetect=True,\n",
    "        ignore_unknown_values=True\n",
    "    )\n",
    "    load_job = client.load_table_from_uri(\n",
    "        full_source_uri,\n",
    "        client.dataset(dataset_name).table(table_name),\n",
    "        job_config=job_config\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        load_job.result()\n",
    "        logging.info(f\"Data loaded from '{full_source_uri}' to table '{dataset_name}.{table_name}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rAnPTd_y78Nw"
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import io\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# Constant values\n",
    "bucket_name = 'gcp_latam_twitter'\n",
    "folder_name = 'raw'\n",
    "zip_file_name = 'tweets.json.zip'\n",
    "\n",
    "# ID de ejemplo del archivo de Google Drive\n",
    "file_id = '1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis'\n",
    "gcloud_url = f\"gs://{bucket_name}/{folder_name}/\"\n",
    "start_time = str(time.time())\n",
    "\n",
    "# Record the start time\n",
    "start_time = str(time.time())\n",
    "\n",
    "\n",
    "# Define local file paths\n",
    "drive_mount_point = '/content/drive/MyDrive'\n",
    "source_path = 'leonardora/de/latam-challenge/'\n",
    "\n",
    "# Set Google Cloud project and dataset info\n",
    "project_id = \"latam-challenge-leonardora\"\n",
    "project_name = \"latam-challenge-leonardora\"\n",
    "dataset = \"latam_tweets_dataset\"\n",
    "table =\"tweets\"\n",
    "json_file_name = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "# Logging\n",
    "logging_level = str(logging.DEBUG)\n",
    "logging.basicConfig(level=int(logging_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUuQovMRznTr",
    "outputId": "dfe62675-8f06-4125-be93-48e03faf2ddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite dataset latam_tweets_dataset\n",
      "Overwrite table latam_tweets_dataset.tweets\n",
      "Store gs://gcp_latam_twitter/raw/farmers-protest-tweets-2021-2-4.json into BigQuery latam_tweets_dataset.tweets\n",
      "Data storage completed!\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# Authenticate to BigQuery\n",
    "bigquery_client: bigquery.Client = authenticate_bigquery(project_id)\n",
    "\n",
    "# Create dataset (overwrite if needed)\n",
    "create_dataset(bigquery_client, dataset, mode='overwrite')\n",
    "print(f\"Overwrite dataset {dataset}\")\n",
    "\n",
    "# Create table (overwrite if needed)\n",
    "create_table(bigquery_client, dataset, table, mode='overwrite')\n",
    "print(f\"Overwrite table {dataset}.{table}\")\n",
    "\n",
    "# Load data from Cloud Storage\n",
    "load_data_from_storage(bigquery_client, gcloud_url, dataset, table, json_file_name)\n",
    "print(f\"Store {gcloud_url}{json_file_name} into BigQuery {dataset}.{table}\")\n",
    "\n",
    "print(\"Data storage completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "C2sokq668ElB"
   },
   "outputs": [],
   "source": [
    "from google.api_core.exceptions import BadRequest, NotFound\n",
    "from google.cloud import bigquery\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "\n",
    "def launch_bigquery(client: bigquery.Client, query: str) -> List[Tuple[Any, Any]]:\n",
    "    data_extracted : List[Tuple[Any, Any]] = []\n",
    "\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "\n",
    "        if results.total_rows == 0:\n",
    "            raise NotFound(\"No results found for the query.\")\n",
    "\n",
    "        data_extracted  = [(row[0], row[1]) for row in results]\n",
    "\n",
    "    except BadRequest as e:\n",
    "        print(f\"BigQuery BadRequest error: {e}\")\n",
    "        raise\n",
    "    except NotFound as e:\n",
    "        print(f\"Query returned no results: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        raise\n",
    "\n",
    "    return data_extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "k4SKIi-uENrw"
   },
   "outputs": [],
   "source": [
    "\n",
    "top_dates_with_top_users = r\"\"\"\n",
    "    WITH\n",
    "    TopDates AS (\n",
    "        SELECT\n",
    "            CAST(date AS DATE) AS tweets_date,\n",
    "            COUNT(id) AS tweet_count\n",
    "        FROM latam_tweets_dataset.tweets\n",
    "        WHERE id IS NOT NULL\n",
    "        GROUP BY tweets_date\n",
    "        ORDER BY tweet_count DESC\n",
    "        LIMIT 10\n",
    "    ),\n",
    "    TopUsersDate AS (\n",
    "        SELECT\n",
    "            TD.tweets_date,\n",
    "            TW.user.username,\n",
    "            MAX(TD.tweet_count) AS max_tweet_count,\n",
    "            COUNT(TW.id) AS user_tweet_count,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY TD.tweets_date\n",
    "                ORDER BY MAX(TD.tweet_count) DESC, COUNT(*) DESC\n",
    "            ) AS row_number\n",
    "        FROM latam_tweets_dataset.tweets AS TW\n",
    "        INNER JOIN TopDates AS TD\n",
    "            ON TD.tweets_date = CAST(TW.date AS DATE)\n",
    "        WHERE TW.id IS NOT NULL\n",
    "        GROUP BY\n",
    "            TD.tweets_date,\n",
    "            TW.user.username\n",
    "        ORDER BY\n",
    "            max_tweet_count DESC,\n",
    "            user_tweet_count DESC,\n",
    "            TW.user.username ASC\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        tweets_date,\n",
    "        username\n",
    "    FROM TopUsersDate\n",
    "    WHERE row_number = 1\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Top 10 most used emojis with their respective counts.\n",
    "Assumption: The query does not contain duplicate entries, as the tweet with 'id = 1362813218952007687' contains two heart and two fist emojis.\n",
    "Assumption: A valid tweet has an id\n",
    "\"\"\"\n",
    "top_emojis = r\"\"\"\n",
    "    WITH\n",
    "    ExtractedEmojis AS (\n",
    "        SELECT\n",
    "            REGEXP_EXTRACT_ALL(\n",
    "                content,\n",
    "                FORMAT(\n",
    "                    r\"(?:[\\x{1F300}-\\x{1F5FF}]|[\\x{1F900}-\\x{1F9FF}]|[\\x{1F600}-\\x{1F64F}]|[\\x{1F680}-\\x{1F6FF}]\" ||\n",
    "                    r\"|[\\x{2600}-\\x{26FF}]\\x{FE0F}?|[\\x{2700}-\\x{27BF}]\\x{FE0F}?|\\x{24C2}\\x{FE0F}?|[\\x{1F1E6}-\\x{1F1FF}]{1,2}\" ||\n",
    "                    r\"|[\\x{1F170}\\x{1F171}\\x{1F17E}\\x{1F17F}\\x{1F18E}\\x{1F191}-\\x{1F19A}]\\x{FE0F}?\" ||\n",
    "                    r\"|[\\\\x{0023}\\x{002A}\\x{0030}-\\x{0039}]\\x{FE0F}?\\x{20E3}|[\\x{2194}-\\x{2199}\\x{21A9}-\\x{21AA}]\\x{FE0F}?\" ||\n",
    "                    r\"|[\\x{2B05}-\\x{2B07}\\x{2B1B}\\x{2B1C}\\x{2B50}\\x{2B55}]\\x{FE0F}?|[\\x{2934}\\x{2935}]\\x{FE0F}?\" ||\n",
    "                    r\"|[\\x{3297}\\x{3299}]\\x{FE0F}?|[\\x{1F201}\\x{1F202}\\x{1F21A}\\x{1F22F}\\x{1F232}\\x{1F23A}\\x{1F250}\\x{1F251}]\\x{FE0F}?\" ||\n",
    "                    r\"|[\\x{203C}-\\x{2049}]\\x{FE0F}?|[\\x{00A9}-\\x{00AE}]\\x{FE0F}?|[\\x{2122}\\x{2139}]\\x{FE0F}?\" ||\n",
    "                    r\"|\\x{1F004}\\x{FE0F}?|\\x{1F0CF}\\x{FE0F}?|[\\x{231A}\\x{231B}\\x{2328}\\x{23CF}\\x{23E9}\\x{23F3}\\x{23F8}\\x{23FA}]\\x{FE0F}?)\"\n",
    "                )\n",
    "            ) AS emojis\n",
    "        FROM latam_tweets_dataset.tweets\n",
    "        WHERE id IS NOT NULL\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        emoji,\n",
    "        COUNT(emoji) AS count\n",
    "    FROM ExtractedEmojis\n",
    "    CROSS JOIN UNNEST(emojis) AS emoji\n",
    "    GROUP BY emoji\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Top 10 all-time most influential users (username) based on the count of mentions (@) each of them receives.\n",
    "Assumption: A valid tweet has an id\n",
    "\"\"\"\n",
    "top_influential_users = r\"\"\"\n",
    "    WITH\n",
    "    MentionedUsersCount AS (\n",
    "        SELECT\n",
    "            user.username AS username,\n",
    "            COUNT(user.username) AS count\n",
    "        FROM\n",
    "            latam_tweets_dataset.tweets as TW,\n",
    "            UNNEST(mentionedUsers) AS user\n",
    "        WHERE TW.id IS NOT NULL\n",
    "        GROUP BY username\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        username,\n",
    "        count AS mention_count\n",
    "    FROM MentionedUsersCount\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "TE3p9TSeFSKb"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import List, Tuple\n",
    "from google.cloud import bigquery\n",
    "import line_profiler\n",
    "\n",
    "\n",
    "@line_profiler.profile\n",
    "def q1_time(client: bigquery.Client, query: str) -> List[Tuple[datetime.date, str]]:\n",
    "  return launch_bigquery(client, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "u2AGolFh-mre",
    "outputId": "8c9660a9-dff6-46e4-ea13-5d2617e869c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATAM Challenge Time - Top 10 Dates with more Tweets and the Username with more Tweets for each Day\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 1.85814 s\n",
      "File: <ipython-input-27-7b5add99afba>\n",
      "Function: q1_time at line 7\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     7                                           @line_profiler.profile\n",
      "     8                                           def q1_time(client: bigquery.Client, query: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9         1 1858141975.0    2e+09    100.0    return launch_bigquery(client, query)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "bigquery_client: bigquery.Client = authenticate_bigquery(project_id)\n",
    "\n",
    "print(\"LATAM Challenge Time - Top 10 Dates with more Tweets and the Username with more Tweets for each Day\")\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(q1_time)\n",
    "profiler.enable_by_count()\n",
    "q1_time_tuple = q1_time(bigquery_client, top_dates_with_top_users)\n",
    "profiler.print_stats()\n",
    "display(q1_time_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "WrOLw3KcFUiG"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from google.cloud import bigquery\n",
    "import line_profiler\n",
    "\n",
    "@line_profiler.profile\n",
    "def q2_time(client: bigquery.Client, query: str) -> List[Tuple[str, int]]:\n",
    "    return launch_bigquery(client, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "r8DM5sPl_GoK",
    "outputId": "8bce66bc-2d8e-4bbd-820a-e9dc9d87ab34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATAM Challenge Time - Top 10 Dates Emojis\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 1.48123 s\n",
      "File: <ipython-input-29-dc45e6cba86f>\n",
      "Function: q2_time at line 5\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     5                                           @line_profiler.profile\n",
      "     6                                           def q2_time(client: bigquery.Client, query: str) -> List[Tuple[str, int]]:\n",
      "     7         1 1481227660.0    1e+09    100.0      return launch_bigquery(client, query)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('✊', 2402),\n",
       " ('❤️', 1382),\n",
       " ('❤', 397),\n",
       " ('☮️', 316),\n",
       " ('♂️', 179),\n",
       " ('✌️', 168),\n",
       " ('♀️', 148),\n",
       " ('✌', 106),\n",
       " ('‼️', 74),\n",
       " ('♥️', 73)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "bigquery_client: bigquery.Client = authenticate_bigquery(project_id)\n",
    "\n",
    "print(\"LATAM Challenge Time - Top 10 Dates Emojis\")\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(q2_time)\n",
    "profiler.enable_by_count()\n",
    "q2_time_tuple = q2_time(bigquery_client, top_emojis)\n",
    "profiler.print_stats()\n",
    "display(q2_time_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Q6b9e7BTFcqf"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from google.cloud import bigquery\n",
    "import line_profiler\n",
    "\n",
    "@line_profiler.profile\n",
    "def q3_time(client: bigquery.Client, query: str) -> List[Tuple[str, int]]:\n",
    "    return launch_bigquery(client, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "ky4QiZPZFMJr",
    "outputId": "65f6e281-22d3-4aaa-97e7-ba10384e2723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATAM Challenge Time - Top 10 Influential Users\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 1.17013 s\n",
      "File: <ipython-input-31-32f9b243065b>\n",
      "Function: q3_time at line 5\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     5                                           @line_profiler.profile\n",
      "     6                                           def q3_time(client: bigquery.Client, query: str) -> List[Tuple[str, int]]:\n",
      "     7         1 1170131743.0    1e+09    100.0      return launch_bigquery(client, query)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "bigquery_client: bigquery.Client = authenticate_bigquery(project_id)\n",
    "\n",
    "print(\"LATAM Challenge Time - Top 10 Influential Users\")\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(q3_time)\n",
    "profiler.enable_by_count()\n",
    "q3_time_tuple = q3_time(bigquery_client, top_influential_users)\n",
    "profiler.print_stats()\n",
    "display(q3_time_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "XdgcjEYeFr1d"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import List, Tuple\n",
    "from google.cloud import bigquery\n",
    "import memory_profiler\n",
    "\n",
    "@memory_profiler.profile\n",
    "def q1_memory(client: bigquery.Client, query: str) -> List[Tuple[datetime.date, str]]:\n",
    "    return launch_bigquery(client, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7UrtGEHFpeq",
    "outputId": "c993088e-55ee-4aa6-d7ce-1ddb12a07857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATAM Challenge Memory - Top 10 Dates with more Tweets and the Username with more Tweets for each Day\n",
      "ERROR: Could not find file <ipython-input-36-2e1c5205567b>\n",
      "NOTE: %mprun can only be used on functions defined in physical files, and not in the IPython environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigquery_client: bigquery.Client = authenticate_bigquery(project_id)\n",
    "print(\"LATAM Challenge Memory - Top 10 Dates with more Tweets and the Username with more Tweets for each Day\")\n",
    "q1_memory(bigquery_client, top_dates_with_top_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "U2OKwHx8FuLb"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from google.cloud import bigquery\n",
    "import memory_profiler\n",
    "\n",
    "@memory_profiler.profile\n",
    "def q2_memory(client: bigquery.Client, query: str) -> List[Tuple[str, int]]:\n",
    "    try:\n",
    "        results = launch_bigquery(client, query)\n",
    "        return [(row[0], int(row[1])) for row in results]\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting data to string and integer pairs: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30qMrZFRGUkW",
    "outputId": "cd0f69a8-2859-4485-a073-dfb208e21e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATAM Challenge Memory - Top 10 Dates Emojis\n",
      "ERROR: Could not find file <ipython-input-38-419bc5c9ce2b>\n",
      "NOTE: %mprun can only be used on functions defined in physical files, and not in the IPython environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('✊', 2402),\n",
       " ('❤️', 1382),\n",
       " ('❤', 397),\n",
       " ('☮️', 316),\n",
       " ('♂️', 179),\n",
       " ('✌️', 168),\n",
       " ('♀️', 148),\n",
       " ('✌', 106),\n",
       " ('‼️', 74),\n",
       " ('♥️', 73)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a title describing the memory usage analysis\n",
    "print(\"LATAM Challenge Memory - Top 10 Dates Emojis\")\n",
    "q2_memory(bigquery_client, top_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "trEaEMhnGaWi"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from google.cloud import bigquery\n",
    "import memory_profiler\n",
    "\n",
    "@memory_profiler.profile\n",
    "def q3_memory(client: bigquery.Client, query: str) -> List[Tuple[str, int]]:\n",
    "   try:\n",
    "       results = launch_bigquery(client, query)\n",
    "       formatted_results = [(row[0], int(row[1])) for row in results]\n",
    "       if len(formatted_results) > 1000:\n",
    "           print(\"Warning: Returning a large dataset. Consider using streaming or pagination for memory optimization.\")\n",
    "       return formatted_results\n",
    "   except ValueError as e:\n",
    "       print(f\"Error converting data to string and integer pairs: {e}\")\n",
    "       return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3Wk6KjRGdtq",
    "outputId": "22ee113b-6306-407a-d435-f7c34e5501a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATAM Challenge Memory - Top 10 Influential Users\n",
      "ERROR: Could not find file <ipython-input-40-629405867ca4>\n",
      "NOTE: %mprun can only be used on functions defined in physical files, and not in the IPython environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"LATAM Challenge Memory - Top 10 Influential Users\")\n",
    "q3_memory(bigquery_client, top_influential_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RussIVlAPIh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
